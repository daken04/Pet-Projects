{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5468571,"sourceType":"datasetVersion","datasetId":534640},{"sourceId":9204337,"sourceType":"datasetVersion","datasetId":5565168}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About dataset\n\n- Data set of 525 bird species. 84635 training images, 2625 test images(5 images per species) and 2625 validation images(5 images per species.\n\n- All images are 224 X 224 X 3 color images in jpg format.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# get the data\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport os\nfrom PIL import Image\nfrom torchvision import transforms\n\n\nclass BirdDataset(Dataset):\n    def __init__(self, csv_file, root_dir, folder='train', transform=None):\n        self.df = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        self.data_frame = self.df[self.df['data set']==folder]\n    \n    def __len__(self):\n        return len(self.data_frame)\n    \n    def __getitem__(self, idx):\n        image_path = os.path.join(self.root_dir, self.data_frame.iloc[idx, 1])\n\n#         if not os.path.exists(image_path):\n#             # You could return a placeholder image, skip this index, or raise a warning\n#             print(f\"File not found: {image_path}, skipping.\")\n#             return None, None\n\n        image = Image.open(image_path)\n        label = self.data_frame.iloc[idx, 0]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n    \ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\ntrain_dataset = BirdDataset(csv_file='/kaggle/input/updated-birds/birds.csv',\n                            root_dir='/kaggle/input/100-bird-species',\n                            folder = 'train',\n                            transform = transform\n                  )\n\ntest_dataset = BirdDataset(csv_file='/kaggle/input/updated-birds/birds.csv',\n                            root_dir='/kaggle/input/100-bird-species',\n                            folder = 'test',\n                            transform = transform\n                  )\n\nvalid_dataset = BirdDataset(csv_file='/kaggle/input/updated-birds/birds.csv',\n                            root_dir='/kaggle/input/100-bird-species',\n                            folder = 'valid',\n                            transform = transform\n                  )\n\n# Get the image and label from the dataset\nimage, label = train_dataset[2]\n\n# Print the shape of the image\nprint(image.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:55:37.588059Z","iopub.execute_input":"2024-08-19T16:55:37.588818Z","iopub.status.idle":"2024-08-19T16:55:44.045003Z","shell.execute_reply.started":"2024-08-19T16:55:37.588787Z","shell.execute_reply":"2024-08-19T16:55:44.043986Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"torch.Size([3, 224, 224])\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n\nfor inputs, labels in train_loader:\n    print(inputs.shape)\n    break\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:55:44.046879Z","iopub.execute_input":"2024-08-19T16:55:44.047235Z","iopub.status.idle":"2024-08-19T16:55:44.365198Z","shell.execute_reply.started":"2024-08-19T16:55:44.047199Z","shell.execute_reply":"2024-08-19T16:55:44.364286Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"torch.Size([32, 3, 224, 224])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n## define the model\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.linear1 = nn.Linear(in_features=64*28*28, out_features=1024, bias=True)\n        self.linear2 = nn.Linear(in_features=1024, out_features=1024, bias=True)\n        self.linear3 = nn.Linear(in_features=1024, out_features=525, bias=True)\n        \n        self.pool = nn.MaxPool2d(kernel_size=2)\n        \n    def forward(self,x):\n        out = self.pool(F.relu(self.conv1(x)))\n        out = self.pool(F.relu(self.conv2(out)))\n        out = self.pool(F.relu(self.conv3(out)))\n        out = out.reshape(out.size(0),-1)\n        out = F.relu(self.linear1(out))\n        out = F.relu(self.linear2(out))\n        out = self.linear3(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:55:44.366485Z","iopub.execute_input":"2024-08-19T16:55:44.367086Z","iopub.status.idle":"2024-08-19T16:55:44.377268Z","shell.execute_reply.started":"2024-08-19T16:55:44.367053Z","shell.execute_reply":"2024-08-19T16:55:44.376255Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## device\nimport torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:55:44.379373Z","iopub.execute_input":"2024-08-19T16:55:44.379792Z","iopub.status.idle":"2024-08-19T16:55:44.413964Z","shell.execute_reply.started":"2024-08-19T16:55:44.379762Z","shell.execute_reply":"2024-08-19T16:55:44.413133Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"## define loss function and optimizer\nmodel = CNNModel()\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nloss_fn = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:55:44.415087Z","iopub.execute_input":"2024-08-19T16:55:44.415426Z","iopub.status.idle":"2024-08-19T16:55:45.129942Z","shell.execute_reply.started":"2024-08-19T16:55:44.415394Z","shell.execute_reply":"2024-08-19T16:55:45.129182Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n## training loop\nepochs = 10\nsee = True\n\nfor epoch in range(epochs):\n    epoch_loss = 0.0\n    num_batches = 0\n    \n    for inputs, labels in tqdm(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        output = model(inputs)  # torch.Size([32, 525])\n        \n        if see:\n            see = False\n            print(output.dtype)\n            print(labels.dtype)\n            \n        # Convert labels to LongTensor\n        labels = labels.long()\n        optimizer.zero_grad()\n        loss = loss_fn(output, labels)\n        loss.backward()\n        \n        optimizer.step()\n        \n        # Accumulate loss\n        epoch_loss += loss.item()\n        num_batches += 1\n    \n    # Calculate average loss for the epoch\n    avg_epoch_loss = epoch_loss / num_batches\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_epoch_loss:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:56:55.954009Z","iopub.execute_input":"2024-08-19T16:56:55.954774Z","iopub.status.idle":"2024-08-19T16:56:59.872098Z","shell.execute_reply.started":"2024-08-19T16:56:55.954739Z","shell.execute_reply":"2024-08-19T16:56:59.870887Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"  0%|          | 1/2645 [00:00<12:26,  3.54it/s]","output_type":"stream"},{"name":"stdout","text":"torch.float32\ntorch.float64\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 14/2645 [00:03<11:53,  3.69it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      9\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     12\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mBirdDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m         image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#         if not os.path.exists(image_path):\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#             # You could return a placeholder image, skip this index, or raise a warning\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#             print(f\"File not found: {image_path}, skipping.\")\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#             return None, None\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3236\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3237\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/100-bird-species/train/PARAKETT AKULET/038.jpg'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/100-bird-species/train/PARAKETT AKULET/038.jpg'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing loop\nmodel.eval()  # Set the model to evaluation mode\ncorrect = 0\ntotal = 0\ntest_loss = 0.0\n\nwith torch.no_grad():  # Disable gradient computation during testing\n    for inputs, labels in tqdm(test_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        labels = labels.long()  # Convert labels to LongTensor\n        \n        output = model(inputs)\n        loss = loss_fn(output, labels)\n        \n        test_loss += loss.item()\n        \n        # Get the class with the highest probability\n        _, predicted = torch.max(output.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n# Calculate the average test loss and accuracy\navg_test_loss = test_loss / len(test_loader)\naccuracy = 100 * correct / total\n\nprint(f'Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:55:48.251486Z","iopub.status.idle":"2024-08-19T16:55:48.251850Z","shell.execute_reply.started":"2024-08-19T16:55:48.251658Z","shell.execute_reply":"2024-08-19T16:55:48.251672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## print one image from test and its label (original, predicted)\n\n","metadata":{},"execution_count":null,"outputs":[]}]}